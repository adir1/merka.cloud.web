---
title: nVidia Learning - Sizing LLM Inference Systems
description: Notes and any Notebook files related to Sizing LLM Inference Systems class from nVidia Learning
---

# Overview



# Notes

## nVidia SW Stack for Inference



## Jargon

- Prefill - time to first Response token (response latency, depends on number of input tokens, KV-cache, compute-bound)
- Decoding - Response inter-token latency (usually memory bound - why?)
- Streaming - Response tokens dynamically stream as they are generated
- FTL - First Token Latency
- NGC - NVIDIA GPU Cloud, is a cloud-based platform that provides software, tools, and services for AI, machine learning, and deep learning. NGC is optimized for scientific computing and GPU acceleration
- 